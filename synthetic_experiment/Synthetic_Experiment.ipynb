{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vflwoF9t88lo"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CENebYst-4DF"
   },
   "outputs": [],
   "source": [
    "def calculate_true_probabilities(x, mu_vec, noise_variance):\n",
    "\n",
    "    N_samples = x.shape[0]\n",
    "    K_classes = mu_vec.shape[0]\n",
    "\n",
    "    log_probabilities = np.zeros((N_samples, K_classes))\n",
    "    for k in range(K_classes):\n",
    "        diff = x - mu_vec[k, :]\n",
    "        squared_distance = np.sum(diff ** 2, axis=1)\n",
    "        log_probabilities[:, k] = -squared_distance / (2 * noise_variance)\n",
    "\n",
    "    probabilities = np.exp(log_probabilities - log_probabilities.max(axis=1, keepdims=True))\n",
    "    probabilities /= probabilities.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "def _y_to_oht(label):\n",
    "  label_oht = torch.zeros(label.shape[0],K_CLAS).to(label.device)\n",
    "  label_oht.scatter_(1,label,1)\n",
    "  label_oht = label_oht.float()\n",
    "  return label_oht\n",
    "\n",
    "def cal_entropy(logits, p):\n",
    "  return nn.KLDivLoss(reduction='batchmean')(F.log_softmax(logits,1), p)\n",
    "\n",
    "def average_l2_distance(tensor1, tensor2):\n",
    "\n",
    "    tensor1_softmax = torch.softmax(tensor1, dim=1)\n",
    "\n",
    "    l2_distances = torch.norm(tensor1_softmax - tensor2, dim=1)\n",
    "\n",
    "    avg_l2_distance = torch.mean(l2_distances).item()\n",
    "\n",
    "    return avg_l2_distance\n",
    "\n",
    "def expected_calibration_error(tensor1, tensor2, n_bins=10):\n",
    "\n",
    "    tensor1_softmax = torch.softmax(tensor1, dim=1)\n",
    "\n",
    "    confidence, predictions = torch.max(tensor1_softmax, dim=1)\n",
    "\n",
    "    _, labels = torch.max(tensor2, dim=1)\n",
    "\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidence > bin_boundaries[i]) & (confidence <= bin_boundaries[i + 1])\n",
    "        prop_in_bin = torch.mean(in_bin.float()).item()\n",
    "\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = torch.mean((predictions[in_bin] == labels[in_bin]).float()).item()\n",
    "            avg_confidence_in_bin = torch.mean(confidence[in_bin]).item()\n",
    "            ece += abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    return ece\n",
    "\n",
    "def normalize_data(data_train, data_val, data_test,return_stats=False,accept_stats=False,mean=0,std=0):\n",
    "\n",
    "  if accept_stats == False:\n",
    "    mean = np.mean(data_train, axis=0)\n",
    "    std = np.std(data_train, axis=0)\n",
    "\n",
    "  data_normalized_train = (data_train - mean) / std\n",
    "  data_normalized_val = (data_val - mean) / std\n",
    "  data_normalized_test = (data_test - mean) / std\n",
    "\n",
    "  if return_stats == False:\n",
    "    return data_normalized_train, data_normalized_val, data_normalized_test\n",
    "\n",
    "  return data_normalized_train, data_normalized_val, data_normalized_test, mean, std\n",
    "\n",
    "def plot_validation_loss(val_losses):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss per Epoch')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.ylim(0,0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_teacher(model, optimizer, x_train, x_val, y_train, y_val, p_train, p_val, p_noisy_train, p_noisy_val, label_type = 'standard', lambd = 1.0, distill = False):\n",
    "\n",
    "    model.train()\n",
    "    seed_torch(42)\n",
    "    length_train = np.shape(x_train)[0]\n",
    "    length_val = np.shape(x_val)[0]\n",
    "    vacc_max = 0\n",
    "    vecep_min = 1000000\n",
    "    vdistp_min = 1000000\n",
    "    results = {'loss': [], 'tacc': [], 'vacc': [], 'tdistp': [], 'vdistp': [], 'tecep': [], 'vecep': [], 'IDX_MAX': [], 'val_loss' : [], 'val_acc' : [] }\n",
    "\n",
    "    x_val_t = torch.Tensor(x_val)\n",
    "    y_val_t = torch.Tensor(y_val).long()\n",
    "    p_val_t = torch.Tensor(p_val)\n",
    "\n",
    "    for j in range(epochs_teacher):\n",
    "        perm = np.random.permutation(length_train)\n",
    "        x_train = x_train[perm]\n",
    "        y_train = y_train[perm]\n",
    "        p_train = p_train[perm]\n",
    "        p_noisy_train = p_noisy_train[perm]\n",
    "\n",
    "        flag = 0\n",
    "        for i in range(0, length_train, batch_size_teacher):\n",
    "            if (i + batch_size_teacher) > length_train:\n",
    "                break\n",
    "            x_in = x_train[i:i+batch_size_teacher, :]\n",
    "            y_in = y_train[i:i+batch_size_teacher, :]\n",
    "            p_in = p_train[i:i+batch_size_teacher, :]\n",
    "            p_in_noisy = p_noisy_train[i:i+batch_size_teacher, :]\n",
    "            x_in = torch.Tensor(x_in)\n",
    "            p_in = torch.Tensor(p_in)\n",
    "            p_in_noisy = torch.Tensor(p_in_noisy)\n",
    "            y_in = torch.Tensor(y_in).long()\n",
    "\n",
    "            x_out = model(x_in)\n",
    "\n",
    "            if label_type == 'gt':\n",
    "              p_tgt = p_in\n",
    "            elif label_type == 'noisy':\n",
    "              p_tgt = p_in_noisy\n",
    "            else:\n",
    "              p_tgt = _y_to_oht(y_in)\n",
    "\n",
    "            if distill == False:\n",
    "              loss = cal_entropy(x_out, p_tgt)\n",
    "            else:\n",
    "              p_tgt1 = _y_to_oht(y_in)\n",
    "              p_tgt2 = p_in_noisy\n",
    "              loss = (1-lambd)*cal_entropy(x_out, p_tgt1) + lambd*cal_entropy(x_out, p_tgt2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            results['loss'].append(loss.item())\n",
    "\n",
    "            tf_table = []\n",
    "            curr_len = np.shape(x_val_t)[0]\n",
    "            with torch.no_grad():\n",
    "              x_val_out = model(x_val_t)\n",
    "              p_tgt_val = _y_to_oht(y_val_t)\n",
    "              val_loss = cal_entropy(x_val_out, p_tgt_val)\n",
    "              x_val_out = x_val_out.detach()\n",
    "\n",
    "            pred = x_val_out.data.max(1, keepdim=True)[1]\n",
    "            tf_table.append(pred.eq(y_val_t.data.view_as(pred)))\n",
    "            tf_table = torch.stack(tf_table).reshape(-1, 1)\n",
    "            correct = tf_table.sum()\n",
    "            valy_accuracy = 100*(correct/curr_len)\n",
    "\n",
    "            results['val_loss'].append(val_loss.item())\n",
    "            results['val_acc'].append(valy_accuracy.item())\n",
    "\n",
    "            if flag == 0:\n",
    "              bayes_loss = nn.KLDivLoss(reduction='batchmean')(p_val_t.log(), p_tgt_val)\n",
    "              print(bayes_loss)\n",
    "              flag = 1\n",
    "\n",
    "        tacc, tdistp, tecep = validate_teacher(model, x_train, y_train, p_train)\n",
    "        vacc, vdistp, vecep = validate_teacher(model, x_val, y_val, p_val)\n",
    "        results['tacc'].append(tacc)\n",
    "        results['vacc'].append(vacc)\n",
    "        results['tdistp'].append(tdistp)\n",
    "        results['vdistp'].append(vdistp)\n",
    "        results['tecep'].append(tecep)\n",
    "        results['vecep'].append(vecep)\n",
    "\n",
    "        if vacc >= vacc_max:\n",
    "            vacc_max = vacc\n",
    "            ES_model = copy.deepcopy(model)\n",
    "            results['IDX_MAX'] = j\n",
    "        if vdistp <= vdistp_min:\n",
    "            vdistp_min = vdistp\n",
    "            ES_model_dist = copy.deepcopy(model)\n",
    "        if vecep <= vecep_min:\n",
    "            vecep_min = vecep\n",
    "            ES_model_ece = copy.deepcopy(model)\n",
    "        if j % 10 == 0:\n",
    "            print('Epoch: {:3d}/{:3d}\\tLoss: {:.3f}\\tTACC: {:.3f},\\tVACC:{:.3f}, \\tVDIST:{:.3f}, \\tVECE:{:.3f}, \\tBestVDIST:{:.3f}, \\tBestVACC:{:.3f}, \\tBestVECE:{:.3f}'.format(j, epochs_teacher, results['loss'][-1], tacc, vacc,vdistp,vecep,vdistp_min,vacc_max, vecep_min))\n",
    "\n",
    "    return ES_model, ES_model_dist, ES_model_ece, results\n",
    "\n",
    "def validate_teacher(model, x, y, p):\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    dist_p = 0\n",
    "    tf_table = []\n",
    "    curr_len = np.shape(x)[0]\n",
    "\n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.Tensor(y).long()\n",
    "    p = torch.Tensor(p)\n",
    "    with torch.no_grad():\n",
    "      x_out = model(x)\n",
    "      x_out = x_out.detach()\n",
    "      pred = x_out.data.max(1, keepdim=True)[1]\n",
    "      tf_table.append(pred.eq(y.data.view_as(pred)))\n",
    "\n",
    "    model.train()\n",
    "    tf_table = torch.stack(tf_table).reshape(-1, 1)\n",
    "    correct = tf_table.sum()\n",
    "    accuracy = 100*(correct/curr_len)\n",
    "\n",
    "    dist_p = average_l2_distance(x_out, p)\n",
    "    ece_p = expected_calibration_error(x_out, p)\n",
    "\n",
    "    return accuracy, dist_p, ece_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cb45iheH-8GN"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, in_dim, hid_size = 40):\n",
    "    super(MLP, self).__init__()\n",
    "    self.in_dim = in_dim\n",
    "    self.hid_size = hid_size\n",
    "    self.fc1 = nn.Linear(self.in_dim, self.hid_size)\n",
    "    self.fc2 = nn.Linear(self.hid_size, self.hid_size)\n",
    "    self.fc3 = nn.Linear(self.hid_size, K_CLAS)\n",
    "    self.act = nn.ReLU(True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1 = self.act(self.fc1(x))\n",
    "    h2 = self.act(self.fc2(h1))\n",
    "    out = self.fc3(h2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8ncaDzX_m3H"
   },
   "outputs": [],
   "source": [
    "def dirichlet_noise_dataset(P, alpha, rng=None, min_prob=1e-12):\n",
    "\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    P = np.asarray(P, dtype=np.float64)\n",
    "\n",
    "    P_safe = np.clip(P, min_prob, None)\n",
    "    P_safe /= P_safe.sum(axis=1, keepdims=True)\n",
    "\n",
    "    if np.isscalar(alpha):\n",
    "        alphas = (alpha * P_safe)\n",
    "    else:\n",
    "        alphas = (np.asarray(alpha)[:, None] * P_safe)\n",
    "\n",
    "    G = rng.gamma(shape=alphas, scale=1.0)\n",
    "    Gsum = G.sum(axis=1, keepdims=True)\n",
    "\n",
    "    mask_bad = (Gsum.squeeze(-1) == 0)\n",
    "    if np.any(mask_bad):\n",
    "        G[mask_bad] = P_safe[mask_bad]\n",
    "        Gsum[mask_bad] = 1.0\n",
    "    return G / Gsum\n",
    "\n",
    "\n",
    "def generate_data_noisy(N_Data, data_split, noise_variance=4,\n",
    "                        label_noise_alpha=None, rng=None):\n",
    "\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    y_true = np.random.randint(0, K_CLAS, [N_Data, 1]).astype(np.float32)\n",
    "    mu_true = np.zeros((N_Data, X_DIM))\n",
    "    for i in range(N_Data):\n",
    "        mu_true[i, :] = MU_VEC[y_true[i].astype(int), :]\n",
    "    x_true = mu_true + np.random.randn(N_Data, X_DIM) * np.sqrt(noise_variance)\n",
    "\n",
    "    p_true = calculate_true_probabilities(x_true, MU_VEC, noise_variance)\n",
    "\n",
    "    indices = np.arange(N_Data)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    x_true = x_true[indices]\n",
    "    y_true = y_true[indices]\n",
    "    p_true = p_true[indices]\n",
    "\n",
    "    N_train = int(N_Data * data_split[0])\n",
    "    N_val = int(N_Data * data_split[1])\n",
    "    N_test = N_Data - N_train - N_val\n",
    "\n",
    "    x_train = x_true[:N_train, :]\n",
    "    x_val   = x_true[N_train:N_train + N_val, :]\n",
    "    x_test  = x_true[N_train + N_val:, :]\n",
    "\n",
    "    y_train = y_true[:N_train]\n",
    "    y_val   = y_true[N_train:N_train + N_val]\n",
    "    y_test  = y_true[N_train + N_val:]\n",
    "\n",
    "    p_train = p_true[:N_train, :]\n",
    "    p_val   = p_true[N_train:N_train + N_val, :]\n",
    "    p_test  = p_true[N_train + N_val:, :]\n",
    "\n",
    "    if label_noise_alpha is not None and label_noise_alpha > 0:\n",
    "        alpha_main = float(label_noise_alpha)\n",
    "        alpha_weak = 0.1 * alpha_main\n",
    "\n",
    "        p_noisy_train = dirichlet_noise_dataset(p_train, alpha_main, rng=rng)\n",
    "        p_noisy_val   = dirichlet_noise_dataset(p_val,   alpha_main, rng=rng)\n",
    "        p_noisy_test  = dirichlet_noise_dataset(p_test,  alpha_main, rng=rng)\n",
    "\n",
    "        p_noisy_train_alpha01 = dirichlet_noise_dataset(p_train, alpha_weak, rng=rng)\n",
    "        p_noisy_val_alpha01   = dirichlet_noise_dataset(p_val,   alpha_weak, rng=rng)\n",
    "        p_noisy_test_alpha01  = dirichlet_noise_dataset(p_test,  alpha_weak, rng=rng)\n",
    "    else:\n",
    "        p_noisy_train = p_train.copy()\n",
    "        p_noisy_val   = p_val.copy()\n",
    "        p_noisy_test  = p_test.copy()\n",
    "\n",
    "        p_noisy_train_alpha01 = p_train.copy()\n",
    "        p_noisy_val_alpha01   = p_val.copy()\n",
    "        p_noisy_test_alpha01  = p_test.copy()\n",
    "\n",
    "    return (x_train, x_val, x_test,\n",
    "            y_train, y_val, y_test,\n",
    "            p_train, p_val, p_test,\n",
    "            p_noisy_train, p_noisy_val, p_noisy_test,\n",
    "            p_noisy_train_alpha01, p_noisy_val_alpha01, p_noisy_test_alpha01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2G8uNUyb9Fz3"
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_torch(2)\n",
    "\n",
    "# Dataset parameters\n",
    "K_CLAS = 5                          # Number of classes in the toy dataset\n",
    "delta_mu = 1                        # Separation between class means\n",
    "NOISE = 2.5                           # Variance of noise (data uncertainty control)\n",
    "label_noise_var = 5                   # Dirichlet noise parameter\n",
    "X_DIM = 30                          # Dimension of input signal x\n",
    "const = 5\n",
    "MU_VEC = np.random.choice([const - delta_mu, const + 0, const + delta_mu], size=(K_CLAS, X_DIM))  # Main class centers\n",
    "\n",
    "# Teacher parameters\n",
    "teacher_split = [0.45, 0.45, 0.1]       # Split ratio between of train/valid/test dataset\n",
    "N_teacher = int(5e4)\n",
    "epochs_teacher = 2\n",
    "batch_size_teacher = 1\n",
    "LR_teacher = 5e-4\n",
    "Hidden_size_teacher = 128\n",
    "\n",
    "initial_model_teacher = MLP(in_dim=X_DIM, hid_size = Hidden_size_teacher)\n",
    "\n",
    "x_train_te, x_val_te, x_test_te, y_train_te, y_val_te, y_test_te, p_train_te, p_val_te, p_test_te, p_noisy_train_te, p_noisy_val_te, p_noisy_test_te, p_noisy1_train_te, p_noisy1_val_te, p_noisy1_test_te = generate_data_noisy(N_teacher, teacher_split, noise_variance=NOISE,label_noise_alpha=label_noise_var)\n",
    "x_train_te_n, x_val_te_n, x_test_te_n, teacher_mean, teacher_std = normalize_data(x_train_te, x_val_te, x_test_te,return_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "OHT_model = copy.deepcopy(initial_model_teacher)\n",
    "OHT_optimizer = optim.SGD(OHT_model.parameters(), lr=LR_teacher, momentum=0.9)\n",
    "best_acc_OHT_model, best_dist_OHT_model, best_ece_OHT_model, OHT_results = train_teacher(OHT_model, OHT_optimizer, x_train_te_n, x_val_te_n, y_train_te, y_val_te, p_train_te, p_val_te, p_noisy_train_te, p_noisy_val_te)"
   ],
   "metadata": {
    "id": "yZOGnhewsrPq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "perf_model = copy.deepcopy(initial_model_teacher)\n",
    "perf_optimizer = optim.SGD(perf_model.parameters(), lr=LR_teacher, momentum=0.9)\n",
    "best_acc_perf_model, best_dist_perf_model, best_ece_perf_model, perf_results = train_teacher(perf_model, perf_optimizer, x_train_te_n, x_val_te_n, y_train_te, y_val_te, p_train_te, p_val_te, p_noisy_train_te, p_noisy_val_te, label_type = 'gt')"
   ],
   "metadata": {
    "id": "NM17fEWrssgx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROa3ZgSqR0Gh"
   },
   "outputs": [],
   "source": [
    "noisy_model = copy.deepcopy(initial_model_teacher)\n",
    "noisy_optimizer = optim.SGD(noisy_model.parameters(), lr=LR_teacher, momentum=0.9)\n",
    "best_acc_noisy_model, best_dist_noisy_model, best_ece_noisy_model, noisy_results = train_teacher(noisy_model, noisy_optimizer, x_train_te_n, x_val_te_n, y_train_te, y_val_te, p_train_te, p_val_te, p_noisy_train_te, p_noisy_val_te, label_type = 'noisy')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "noisy2_model = copy.deepcopy(initial_model_teacher)\n",
    "noisy2_optimizer = optim.SGD(noisy2_model.parameters(), lr=LR_teacher, momentum=0.9)\n",
    "best_acc_noisy2_model, best_dist_noisy2_model, best_ece_noisy2_model, noisy2_results = train_teacher(noisy2_model, noisy2_optimizer, x_train_te_n, x_val_te_n, y_train_te, y_val_te, p_train_te, p_val_te, p_noisy1_train_te, p_noisy1_val_te, label_type = 'noisy')"
   ],
   "metadata": {
    "id": "rREyEbNGstpR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def save_family(prefix):\n",
    "    acc_name   = f\"best_acc_{prefix}_model\"\n",
    "    dist_name  = f\"best_dist_{prefix}_model\"\n",
    "    ece_name   = f\"best_ece_{prefix}_model\"\n",
    "    res_name   = f\"{prefix}_results\"\n",
    "\n",
    "    acc_model   = globals()[acc_name]\n",
    "    dist_model  = globals()[dist_name]\n",
    "    ece_model   = globals()[ece_name]\n",
    "    results_obj = globals()[res_name]\n",
    "\n",
    "    torch.save(acc_model.state_dict(),  f\"{save_dir}/{acc_name}.pth\")\n",
    "    torch.save(dist_model.state_dict(), f\"{save_dir}/{dist_name}.pth\")\n",
    "    torch.save(ece_model.state_dict(),  f\"{save_dir}/{ece_name}.pth\")\n",
    "    with open(f\"{save_dir}/{res_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results_obj, f)\n",
    "    print(f\"Saved {prefix}\")\n",
    "\n",
    "for fam in [\"perf\", \"noisy\", \"noisy2\", \"OHT\"]:\n",
    "    save_family(fam)"
   ],
   "metadata": {
    "id": "BFYlwqU0svZ1"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
